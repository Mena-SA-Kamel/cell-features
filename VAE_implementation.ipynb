{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mena-SA-Kamel/cell-features/blob/feature%2Fvae/VAE_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Variational Autoencoder to Extract Feature Embeddings from MNIST"
      ],
      "metadata": {
        "id": "twHRQEW3i8VY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SRfQmCCxi1ba"
      },
      "outputs": [],
      "source": [
        "# Python helpers\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Torch model imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Torch dataset imports\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Torch data loader + transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Plotting functions\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Transforms"
      ],
      "metadata": {
        "id": "wl2CuU2okkaH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kHlofVlskjbT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Creating an iterator for the training data\n",
        "# num_samples = 25\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "# sample_images = [images[i,0] for i in range(num_samples)]\n",
        "\n",
        "# fig = plt.figure(figsize=(5, 5))\n",
        "# grid = ImageGrid(fig, 111, nrows_ncols=(5, 5), axes_pad=0.1)\n",
        "\n",
        "# for ax, im in zip(grid, sample_images):\n",
        "#     ax.imshow(im, cmap='gray')\n",
        "#     ax.axis('off')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "xIUlXpABlrsw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the VAE class"
      ],
      "metadata": {
        "id": "HDN9nZb-mxql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# download the MNIST datasets\n",
        "path = '/content/datasets'\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "\n",
        "train_dataset = MNIST(path, transform=transform, download=True)\n",
        "test_dataset  = MNIST(path, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)\n",
        "# New VAE Class\n",
        "class VariationalEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "    super(VariationalEncoder, self).__init__()\n",
        "    # TODO: Change linear layers to Conv layers\n",
        "    self.hidden_layer_1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.hidden_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.mean_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.N = torch.distributions.Normal(0, 1) # Gaussian normalization\n",
        "    self.N.loc = self.N.loc.cuda() # Moving the loc/ mean of the distribution to CUDA\n",
        "    self.N.scale = self.N.scale.cuda() # Moving the scale/ std of the distribution to CUDA\n",
        "    self.KL = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, start_dim=1) # Flatten the image\n",
        "    x = nn.functional.leaky_relu(self.hidden_layer_1(x), 0.2)\n",
        "    x = nn.functional.leaky_relu(self.hidden_layer_2(x), 0.2)\n",
        "    means = self.mean_layer(x)\n",
        "    log_vars = torch.exp(self.logvar_layer(x)) # eNetwork predicting log(sigma **2) = log(var)\n",
        "    # latent distribution using reparameterization trick to enable backpropagation\n",
        "    var = torch.exp(0.5*log_vars) # torch.exp(0.5*log_vars) -> sqrt(var) = sigma\n",
        "    epsilon = torch.randn_like(var).to(device)\n",
        "    z = means + (epsilon * var)\n",
        "    # KL divergence\n",
        "    self.KL = (-0.5*(1 + log_vars - means**2 - torch.exp(log_vars))).sum() # torch.exp(log_vars)) -> var = sigma **2\n",
        "    return z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_layer_1 = nn.Linear(latent_dim, hidden_dim)\n",
        "    self.hidden_layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, z):\n",
        "    z = nn.functional.leaky_relu(self.hidden_layer_1(z), 0.2)\n",
        "    z = nn.functional.leaky_relu(self.hidden_layer_2(z), 0.2)\n",
        "    z = torch.sigmoid(self.output_layer(z)) # So it outputs a numpber between 0-1\n",
        "    return z.reshape((-1, 1, 28, 28))\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "    super(VariationalAutoEncoder, self).__init__()\n",
        "    self.encoder = VariationalEncoder(input_dim, hidden_dim, latent_dim)\n",
        "    self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.encoder(x)\n",
        "    x_hat = self.decoder(z)\n",
        "    return x_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STw8Md25nG5w",
        "outputId": "7b7091b3-2b41-4490-959f-570585792e78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(vae, data, epochs=20):\n",
        "  optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "  for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    for batch_id, (x, _) in enumerate(data):\n",
        "      x = x.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      x_hat = vae.forward(x)\n",
        "      # reconstruction_loss = ((x - x_hat)**2).sum()\n",
        "      reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "      loss = reconstruction_loss + vae.encoder.KL\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      overall_loss += loss.item()\n",
        "    if batch_id > 0:\n",
        "      print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss / (batch_id*batch_size), \"Reconstruction loss: \", reconstruction_loss.item(), \"KL: \", vae.encoder.KL.item())\n",
        "  return vae\n",
        "\n",
        "vae = VariationalAutoEncoder(input_dim=784, hidden_dim=100, latent_dim=2).to(device)\n",
        "vae = train(vae, train_loader, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLNW6LFYEEPm",
        "outputId": "481d10dd-f4d3-49d9-885f-03c2d49f76c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 1 \tAverage Loss:  204.52451295779423 Reconstruction loss:  17832.861328125 KL:  675.8216552734375\n",
            "\tEpoch 2 \tAverage Loss:  181.44023597271493 Reconstruction loss:  17897.1328125 KL:  621.22509765625\n",
            "\tEpoch 3 \tAverage Loss:  178.916421509808 Reconstruction loss:  16577.244140625 KL:  878.2794189453125\n",
            "\tEpoch 4 \tAverage Loss:  177.97950819725585 Reconstruction loss:  17025.02734375 KL:  763.89892578125\n",
            "\tEpoch 5 \tAverage Loss:  177.09695480423102 Reconstruction loss:  17480.927734375 KL:  693.816162109375\n",
            "\tEpoch 6 \tAverage Loss:  176.6116567162719 Reconstruction loss:  16558.87890625 KL:  867.126953125\n",
            "\tEpoch 7 \tAverage Loss:  176.1941649167884 Reconstruction loss:  16951.091796875 KL:  1011.36376953125\n",
            "\tEpoch 8 \tAverage Loss:  175.78903286075752 Reconstruction loss:  16460.099609375 KL:  940.6197509765625\n",
            "\tEpoch 9 \tAverage Loss:  175.48003783975898 Reconstruction loss:  17237.45703125 KL:  918.5283203125\n",
            "\tEpoch 10 \tAverage Loss:  175.2300429915745 Reconstruction loss:  16456.3671875 KL:  839.3871459960938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reconstructed(autoencoder, r0=(-10, 10), r1=(-10, 10), n=12):\n",
        "    w = 28\n",
        "    img = np.zeros((n*w, n*w))\n",
        "    for i, y in enumerate(np.linspace(*r1, n)):\n",
        "        for j, x in enumerate(np.linspace(*r0, n)):\n",
        "            z = torch.Tensor([[x, y]]).to(device)\n",
        "            x_hat = autoencoder.decoder(z)\n",
        "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
        "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
        "    plt.imshow(img, extent=[*r0, *r1])\n",
        "plot_reconstructed(vae)\n"
      ],
      "metadata": {
        "id": "p_Rd1WEdt3ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent(autoencoder, data, num_batches=100):\n",
        "    for i, (x, y) in enumerate(data):\n",
        "        z = autoencoder.encoder(x.to(device))\n",
        "        z = z.to('cpu').detach().numpy()\n",
        "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
        "        if i > num_batches:\n",
        "            plt.colorbar()\n",
        "            break\n",
        "plot_latent(vae, train_loader)"
      ],
      "metadata": {
        "id": "rMfXmyGdt3by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class VAE(nn.Module):\n",
        "#   def __init__(self, input_dim=784, hidden_dim=400, latent_dim=200, device=device):\n",
        "#     super(VAE, self).__init__()\n",
        "\n",
        "#     #Encoder\n",
        "#     self.encoder = nn.Sequential(\n",
        "#         nn.Linear(input_dim, hidden_dim),\n",
        "#         nn.LeakyReLU(0.2),\n",
        "#         nn.Linear(hidden_dim, latent_dim),\n",
        "#         nn.LeakyReLU(0.2)\n",
        "#     )\n",
        "\n",
        "#     # Latent mean and variance\n",
        "#     self.mean_layer = nn.Linear(latent_dim, 1)\n",
        "#     self.logvar_layer = nn.Linear(latent_dim, 1)\n",
        "\n",
        "#     # Decoder\n",
        "#     self.decoder = nn.Sequential(\n",
        "#         nn.Linear(1, latent_dim),\n",
        "#         nn.LeakyReLU(0.2),\n",
        "#         nn.Linear(latent_dim, hidden_dim),\n",
        "#         nn.LeakyReLU(0.2),\n",
        "#         nn.Linear(hidden_dim, input_dim),\n",
        "#         nn.Sigmoid()\n",
        "#     )\n",
        "\n",
        "#   def encode(self, x):\n",
        "#     x = self.encoder(x)\n",
        "#     mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
        "#     return mean, logvar\n",
        "\n",
        "#   def reparametrization(self, mean, var):\n",
        "#     epsilon = torch.randn_like(var).to(device)\n",
        "#     z = mean + var*epsilon\n",
        "#     return z\n",
        "\n",
        "#   def decode(self, x):\n",
        "#     return self.decoder(x)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     mean, logvar = self.encode(x)\n",
        "#     z = self.reparametrization(mean, logvar)\n",
        "#     x_hat = self.decode(z)\n",
        "#     return x_hat, mean, logvar\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JcSwMWIKmv0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "def train(model, optimizer, epochs, device, train_loader, x_dim=784):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "      overall_loss = 0\n",
        "      for batch_idx, (x,_) in enumerate(train_loader):\n",
        "        x = x.view(batch_size, x_dim).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        loss = loss_function(x, x_hat,mean, log_var)\n",
        "        overall_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
        "    return overall_loss"
      ],
      "metadata": {
        "id": "-d0fGueps2Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %env CUDA_LAUNCH_BLOCKING=1\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "print (device)\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer, epochs=50, device=device, train_loader=train_loader)"
      ],
      "metadata": {
        "id": "KlP_z8GWpkl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_digit(mean, var):\n",
        "    z_sample = torch.tensor([[mean]], dtype=torch.float).to(device)\n",
        "    x_decoded = model.decode(z_sample)\n",
        "    digit = x_decoded.detach().cpu().reshape(28, 28) # reshape vector to 2d array\n",
        "    plt.title(f'[{mean},{var}]')\n",
        "    plt.imshow(digit, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YKBFccIFzmBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_digit(0.005, 0.2)"
      ],
      "metadata": {
        "id": "2SKhOOmLusJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}